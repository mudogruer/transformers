{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30648,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "chess prediction with transformers",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mudogruer/transformers/blob/main/chess_prediction_with_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CAN AUTO-REGRESSION MODELS GENERATE A CHESS GAME\n"
      ],
      "metadata": {
        "id": "DweCMvc1NsPw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I though that an decoder language model can create a logical chess game while I am precticing on NLP. Then I want to create a model to see the result.\n",
        "Let's see some funny results and learn about decoders."
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-05T19:52:47.481782Z",
          "iopub.execute_input": "2024-02-05T19:52:47.483053Z",
          "iopub.status.idle": "2024-02-05T19:52:47.49017Z",
          "shell.execute_reply.started": "2024-02-05T19:52:47.483015Z",
          "shell.execute_reply": "2024-02-05T19:52:47.48872Z"
        },
        "id": "2yzmC3eJNsPy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First download libraries that we need."
      ],
      "metadata": {
        "id": "GOpX8PnsNsPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U accelerate datasets transformers"
      ],
      "metadata": {
        "trusted": true,
        "id": "RdX8oUFmNsP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I've found a dataset on HuggingFace which includes 360k chess game. But I cannot use this amount of game because of RAM capacity.\n",
        "Max. %5 of it can be used regarding my trying."
      ],
      "metadata": {
        "id": "klzoeVZpNsP1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset('mlabonne/chessllm',split='train[95%:]')\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-05T19:06:04.259426Z",
          "iopub.execute_input": "2024-02-05T19:06:04.259849Z",
          "iopub.status.idle": "2024-02-05T19:06:07.439215Z",
          "shell.execute_reply.started": "2024-02-05T19:06:04.259813Z",
          "shell.execute_reply": "2024-02-05T19:06:07.43836Z"
        },
        "trusted": true,
        "id": "dSR_63u-NsP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload the data and see what we have here."
      ],
      "metadata": {
        "id": "l0Mt8m_3NsP3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)\n",
        "print(dataset[0])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-05T20:31:39.804175Z",
          "iopub.execute_input": "2024-02-05T20:31:39.805011Z",
          "iopub.status.idle": "2024-02-05T20:31:40.600991Z",
          "shell.execute_reply.started": "2024-02-05T20:31:39.804974Z",
          "shell.execute_reply": "2024-02-05T20:31:40.599476Z"
        },
        "trusted": true,
        "id": "6-h80J_TNsP4",
        "outputId": "63f79fcf-1249-4c74-f220-2e942f0d6ba0"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "DatasetDict({\n    train: Dataset({\n        features: ['average_elo', 'text'],\n        num_rows: 16230\n    })\n})\n",
          "output_type": "stream"
        },
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[59], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(dataset)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/dataset_dict.py:80\u001b[0m, in \u001b[0;36mDatasetDict.__getitem__\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m     76\u001b[0m available_suggested_splits \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     77\u001b[0m     split \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m (Split\u001b[38;5;241m.\u001b[39mTRAIN, Split\u001b[38;5;241m.\u001b[39mTEST, Split\u001b[38;5;241m.\u001b[39mVALIDATION) \u001b[38;5;28;01mif\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m     78\u001b[0m ]\n\u001b[1;32m     79\u001b[0m suggested_split \u001b[38;5;241m=\u001b[39m available_suggested_splits[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m available_suggested_splits \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 80\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid key: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Please first select a split. For example: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`my_dataset_dictionary[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuggested_split\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m][\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAvailable splits: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     84\u001b[0m )\n",
            "\u001b[0;31mKeyError\u001b[0m: \"Invalid key: 0. Please first select a split. For example: `my_dataset_dictionary['train'][0]`. Available splits: ['train']\""
          ],
          "ename": "KeyError",
          "evalue": "\"Invalid key: 0. Please first select a split. For example: `my_dataset_dictionary['train'][0]`. Available splits: ['train']\"",
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The last 3 character is about the results. We dont need them here. Lets create a column for them."
      ],
      "metadata": {
        "id": "kJL2y7TINsP5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_scores(example):\n",
        "    # Extract the last 3 characters from the 'transcript' field\n",
        "    example['scores'] = example['transcript'][-3:]\n",
        "    example['transcript'] = example['transcript'][:-3]\n",
        "    return example\n",
        "\n",
        "# Apply the function to each example in the dataset\n",
        "dataset = dataset.map(lambda examples: extract_scores(examples), batched=False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-05T19:06:54.746451Z",
          "iopub.execute_input": "2024-02-05T19:06:54.74683Z",
          "iopub.status.idle": "2024-02-05T19:06:55.861748Z",
          "shell.execute_reply.started": "2024-02-05T19:06:54.746803Z",
          "shell.execute_reply": "2024-02-05T19:06:55.860693Z"
        },
        "trusted": true,
        "id": "hg1krANZNsP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-05T19:06:57.381784Z",
          "iopub.execute_input": "2024-02-05T19:06:57.382188Z",
          "iopub.status.idle": "2024-02-05T19:06:57.388818Z",
          "shell.execute_reply.started": "2024-02-05T19:06:57.382155Z",
          "shell.execute_reply": "2024-02-05T19:06:57.387676Z"
        },
        "trusted": true,
        "id": "Wh_NFA5wNsP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, I want to record scores in another column as integer to analyze."
      ],
      "metadata": {
        "id": "2dlDU5AaNsP9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_label_column(example):\n",
        "    # Extract the last 3 characters from the 'transcript' field\n",
        "    if example['scores'] == '1-0':\n",
        "        example['label'] = 1\n",
        "    elif example['scores'] == '1/2':\n",
        "        example['label'] = 0\n",
        "    elif example['scores'] == '0-1':\n",
        "        example['label'] = -1\n",
        "    else:\n",
        "        example['label']= None\n",
        "    return example\n",
        "\n",
        "# Apply the function to each example in the dataset\n",
        "dataset = dataset.map(lambda examples: create_label_column(examples), batched=False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-05T19:06:59.652805Z",
          "iopub.execute_input": "2024-02-05T19:06:59.653757Z",
          "iopub.status.idle": "2024-02-05T19:07:00.917933Z",
          "shell.execute_reply.started": "2024-02-05T19:06:59.653722Z",
          "shell.execute_reply": "2024-02-05T19:07:00.91686Z"
        },
        "trusted": true,
        "id": "o9TBO_g4NsP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-05T19:07:02.569089Z",
          "iopub.execute_input": "2024-02-05T19:07:02.569512Z",
          "iopub.status.idle": "2024-02-05T19:07:02.57691Z",
          "shell.execute_reply.started": "2024-02-05T19:07:02.569469Z",
          "shell.execute_reply": "2024-02-05T19:07:02.575872Z"
        },
        "trusted": true,
        "id": "6EADWjaUNsP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import *\n",
        "\n",
        "dataset = DatasetDict({\"train\":dataset})"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-05T19:07:04.281918Z",
          "iopub.execute_input": "2024-02-05T19:07:04.282844Z",
          "iopub.status.idle": "2024-02-05T19:07:04.287294Z",
          "shell.execute_reply.started": "2024-02-05T19:07:04.282809Z",
          "shell.execute_reply": "2024-02-05T19:07:04.286321Z"
        },
        "trusted": true,
        "id": "rXhIzcE2NsP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"train\"].features"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-05T19:07:05.747819Z",
          "iopub.execute_input": "2024-02-05T19:07:05.748559Z",
          "iopub.status.idle": "2024-02-05T19:07:05.755564Z",
          "shell.execute_reply.started": "2024-02-05T19:07:05.748521Z",
          "shell.execute_reply": "2024-02-05T19:07:05.754529Z"
        },
        "trusted": true,
        "id": "yURL2WQSNsP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Everthing looks good. Now lets make them dataframe and go deeper"
      ],
      "metadata": {
        "id": "GCeDX3EJNsQA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as ps\n",
        "\n",
        "dataset.set_format(type=\"pandas\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-05T19:07:07.488977Z",
          "iopub.execute_input": "2024-02-05T19:07:07.489407Z",
          "iopub.status.idle": "2024-02-05T19:07:07.495058Z",
          "shell.execute_reply.started": "2024-02-05T19:07:07.489373Z",
          "shell.execute_reply": "2024-02-05T19:07:07.49391Z"
        },
        "trusted": true,
        "id": "UYl10y7nNsQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df= dataset[\"train\"][:]\n",
        "df.head(10)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-05T19:07:08.923823Z",
          "iopub.execute_input": "2024-02-05T19:07:08.924244Z",
          "iopub.status.idle": "2024-02-05T19:07:08.960713Z",
          "shell.execute_reply.started": "2024-02-05T19:07:08.92421Z",
          "shell.execute_reply": "2024-02-05T19:07:08.959647Z"
        },
        "trusted": true,
        "id": "eDprZWc5NsQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-05T19:07:10.783554Z",
          "iopub.execute_input": "2024-02-05T19:07:10.784998Z",
          "iopub.status.idle": "2024-02-05T19:07:10.789947Z",
          "shell.execute_reply.started": "2024-02-05T19:07:10.78495Z",
          "shell.execute_reply": "2024-02-05T19:07:10.788501Z"
        },
        "trusted": true,
        "id": "iS3CGkqINsQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"label\"].value_counts(ascending = True).plot.barh()\n",
        "plt.title(\"Frequency of Scores\")\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-05T19:07:12.324726Z",
          "iopub.execute_input": "2024-02-05T19:07:12.325139Z",
          "iopub.status.idle": "2024-02-05T19:07:12.537633Z",
          "shell.execute_reply.started": "2024-02-05T19:07:12.325108Z",
          "shell.execute_reply": "2024-02-05T19:07:12.536534Z"
        },
        "trusted": true,
        "id": "e9XEvmhbNsQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Whites have advantages. That makes sense."
      ],
      "metadata": {
        "id": "wEMetCMgNsQC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"tokens_per_game\"] = df[\"transcript\"].str.split().apply(len)\n",
        "df.boxplot(\"tokens_per_game\", by = \"label\", grid = False,showfliers=False, color = \"black\")\n",
        "plt.suptitle(\"\")\n",
        "plt.xlabel(\"\")\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-05T19:07:14.27669Z",
          "iopub.execute_input": "2024-02-05T19:07:14.277081Z",
          "iopub.status.idle": "2024-02-05T19:07:14.926513Z",
          "shell.execute_reply.started": "2024-02-05T19:07:14.277051Z",
          "shell.execute_reply": "2024-02-05T19:07:14.925486Z"
        },
        "trusted": true,
        "id": "gxlt9KPcNsQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data looks ok. Goodbye dataframe"
      ],
      "metadata": {
        "id": "1c_zFLVMNsQD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.reset_format()\n",
        "dataset"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-05T19:07:16.475049Z",
          "iopub.execute_input": "2024-02-05T19:07:16.475467Z",
          "iopub.status.idle": "2024-02-05T19:07:16.485242Z",
          "shell.execute_reply.started": "2024-02-05T19:07:16.475418Z",
          "shell.execute_reply": "2024-02-05T19:07:16.484158Z"
        },
        "trusted": true,
        "id": "07_p7iCWNsQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to use decoder model. So, we dont need label or any classification thing. We cannot rule this model. He is the boss here. <br>\n",
        "Because of this, we need to remove scores."
      ],
      "metadata": {
        "id": "AfnAVJ2BNsQE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset[\"train\"].rename_column(\"transcript\",\"text\")\n",
        "dataset"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-05T19:07:18.0705Z",
          "iopub.execute_input": "2024-02-05T19:07:18.071312Z",
          "iopub.status.idle": "2024-02-05T19:07:18.081084Z",
          "shell.execute_reply.started": "2024-02-05T19:07:18.07128Z",
          "shell.execute_reply": "2024-02-05T19:07:18.079915Z"
        },
        "trusted": true,
        "id": "3TSD5YIXNsQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = DatasetDict({\"train\":dataset})"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-05T19:07:19.602629Z",
          "iopub.execute_input": "2024-02-05T19:07:19.603087Z",
          "iopub.status.idle": "2024-02-05T19:07:19.608Z",
          "shell.execute_reply.started": "2024-02-05T19:07:19.603054Z",
          "shell.execute_reply": "2024-02-05T19:07:19.606884Z"
        },
        "trusted": true,
        "id": "yyyd31x_NsQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.remove_columns(\"scores\")\n",
        "dataset = dataset.remove_columns(\"label\")\n",
        "dataset"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-05T19:07:21.106862Z",
          "iopub.execute_input": "2024-02-05T19:07:21.107274Z",
          "iopub.status.idle": "2024-02-05T19:07:21.119892Z",
          "shell.execute_reply.started": "2024-02-05T19:07:21.107246Z",
          "shell.execute_reply": "2024-02-05T19:07:21.118794Z"
        },
        "trusted": true,
        "id": "c-b5cJm2NsQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"train\"].features"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-05T19:07:22.994551Z",
          "iopub.execute_input": "2024-02-05T19:07:22.99534Z",
          "iopub.status.idle": "2024-02-05T19:07:23.001917Z",
          "shell.execute_reply.started": "2024-02-05T19:07:22.995305Z",
          "shell.execute_reply": "2024-02-05T19:07:23.000764Z"
        },
        "trusted": true,
        "id": "TOgEI9ZeNsQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We understand data. But model cannot speak this language. Lets speak in his language. <br>\n",
        "Let the tokenization begin!"
      ],
      "metadata": {
        "id": "b2mh1fCwNsQG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(batch):\n",
        "  return tokenizer(batch[\"text\"],truncation=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-05T19:07:26.286386Z",
          "iopub.execute_input": "2024-02-05T19:07:26.28746Z",
          "iopub.status.idle": "2024-02-05T19:07:26.295125Z",
          "shell.execute_reply.started": "2024-02-05T19:07:26.287404Z",
          "shell.execute_reply": "2024-02-05T19:07:26.292549Z"
        },
        "trusted": true,
        "id": "eZhf7SUDNsQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Of course, I will push the hub this model. Maybe someone inspire of it :P"
      ],
      "metadata": {
        "id": "SgPhAq8FNsQH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-05T19:30:45.1383Z",
          "iopub.execute_input": "2024-02-05T19:30:45.138776Z",
          "iopub.status.idle": "2024-02-05T19:30:45.168159Z",
          "shell.execute_reply.started": "2024-02-05T19:30:45.138741Z",
          "shell.execute_reply": "2024-02-05T19:30:45.167151Z"
        },
        "trusted": true,
        "id": "e_hWSr0_NsQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose to use GPT2 because why not. We define training arguments and other requirements."
      ],
      "metadata": {
        "id": "F-EfrstRNsQI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "import torch\n",
        "\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n",
        "\n",
        "# Assume dataset_encoded is correctly tokenized and prepared\n",
        "dataset_encoded = dataset.map(tokenize, batched=True, batch_size=None)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=1,\n",
        "    optim=\"adamw_hf\",  # Corrected optimizer name\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=25,\n",
        "    logging_steps=25,\n",
        "    learning_rate=2e-3,\n",
        "    weight_decay=0.001,\n",
        "    max_steps=50,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset_encoded[\"train\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-05T19:07:34.260803Z",
          "iopub.execute_input": "2024-02-05T19:07:34.261767Z",
          "iopub.status.idle": "2024-02-05T19:08:33.998448Z",
          "shell.execute_reply.started": "2024-02-05T19:07:34.261726Z",
          "shell.execute_reply": "2024-02-05T19:08:33.997476Z"
        },
        "trusted": true,
        "id": "bOc8txvrNsQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training was over and I was able to train 50 steps due to physical disabilities. Let's see what a little trained decoder model can do."
      ],
      "metadata": {
        "id": "Fpd6LXXkNsQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Generate a chess game begin with these moves:\"+\"1. e4 e5 2. d4 d5 3. exd5 exd4 4. Nf3 Bf5\"  # Your chess move sequence\n",
        "\n",
        "# Tokenize the prompt to input IDs\n",
        "input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
        "\n",
        "# Generate text using the model. Adjust the parameters as needed.\n",
        "max_length = input_ids.shape[1] + 25  # You can adjust the length of the prediction\n",
        "temperature = 0.5  # Controls the randomness: lower means less random\n",
        "num_return_sequences = 1  # Number of sequences to generate\n",
        "\n",
        "# Generate predictions\n",
        "output_sequences = model.generate(\n",
        "    input_ids=input_ids,\n",
        "    max_length=max_length,\n",
        "    temperature=temperature,\n",
        "    top_k=5,\n",
        "    top_p=0.95,\n",
        "    repetition_penalty=0.5,\n",
        "    do_sample=True,\n",
        "    num_return_sequences=num_return_sequences,\n",
        ")\n",
        "\n",
        "# Decode the generated sequences to text\n",
        "generated_sequences = []\n",
        "for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n",
        "    generated_sequence = generated_sequence.tolist()\n",
        "\n",
        "    # Decode text\n",
        "    text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n",
        "\n",
        "    # Remove all text after the stop token\n",
        "    text = text[: text.find(tokenizer.eos_token) if tokenizer.eos_token else None]\n",
        "\n",
        "    # Add the prompt at the beginning of the sequence. Remove the prompt from the generated sequence if not desired.\n",
        "    total_sequence = (\n",
        "        prompt + text[len(tokenizer.decode(input_ids[0], clean_up_tokenization_spaces=True)) :]\n",
        "    )\n",
        "\n",
        "    generated_sequences.append(total_sequence)\n",
        "\n",
        "for generated_sequence in generated_sequences:\n",
        "    print(generated_sequence)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-05T19:29:01.324905Z",
          "iopub.execute_input": "2024-02-05T19:29:01.32591Z",
          "iopub.status.idle": "2024-02-05T19:29:01.727472Z",
          "shell.execute_reply.started": "2024-02-05T19:29:01.325872Z",
          "shell.execute_reply": "2024-02-05T19:29:01.726463Z"
        },
        "trusted": true,
        "id": "M4yVgDC8NsQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# my_finetuned_model = \"gpt2-chess-move-prediction\"\n",
        "\n",
        "# trainer.model.push_to_hub(my_finetuned_model)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-05T19:32:00.667732Z",
          "iopub.execute_input": "2024-02-05T19:32:00.668173Z",
          "iopub.status.idle": "2024-02-05T19:32:22.181882Z",
          "shell.execute_reply.started": "2024-02-05T19:32:00.668139Z",
          "shell.execute_reply": "2024-02-05T19:32:22.180751Z"
        },
        "trusted": true,
        "id": "6DFrOnvmNsQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CONCLUSION\n",
        "\n",
        "By its nature, there is no such thing as unreasonable as expecting logical results from decoder models. But they say curiosity killed the cat, so I was curious and tried it. While 1 or 2 moves may be logical, the other moves consist of completely illogical moves. Still, it looked real enough to fool someone who didn't know chess.\n",
        "\n",
        "God knows how much more logical the answers would be if I could use the entire data set and run more epochs. Of course, something can be found by playing on the educational arguments.\n",
        "\n",
        "What this idle study contributed to me was to better understand the working logic of decoder models. It would be better if we leave the chess job to reinforcement learning algorithms."
      ],
      "metadata": {
        "id": "M43TsF2HNsQK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qY47YJBLNsQL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}